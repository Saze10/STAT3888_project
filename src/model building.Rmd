---
title: "Untitled"
author: 'SID: 490300286'
date: "21/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message = F, warning  = F, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyr)     # new tidy functions
library(naniar)

load("tech_data_biom_and_nutr.Rdata")
```

```{r}
dat = tech_biom

# link nutrient data with biomedical data

macros = tech_nutr %>% select(ABSPID,
                              PROTT1,
                              PROTT2,
                              FATT1,
                              FATT2,
                              CHOWSAT1,
                              CHOWSAT2)


# gettin average of macros for each person
macros$avg_protein <- rowMeans(macros[ , c(2,3)], na.rm=TRUE)
macros$avg_fat <- rowMeans(macros[ , c(4,5)], na.rm=TRUE)
macros$avg_carb <- rowMeans(macros[ , c(6,7)], na.rm=TRUE)

# making macros1 only df of averages
macros1 = macros %>% select(-c(2:7))
macros2 = macros1

# currently all measurements are in grams, so convert to calories
# using fact that 1g protein or carb = 4 cal and 
# 1g fat = 9 calories (reference: https://doi.org/10.1093/aje/128.5.1065)

macros2$avg_protein = macros2$avg_protein*4
macros2$avg_carb = macros2$avg_carb*4
macros2$avg_fat = macros2$avg_fat*9


###################### NOTE: Cannot use percentages in a linear model as matrix will be singular so there will be no coefficient for fat


# 
# # trying to find % of each macro
# 
# macros2$pct_protein <- NA
# macros2$pct_fat <- NA
# macros2$pct_carb <- NA
# 
# for (i in 1:nrow(macros2)) {
#   macros2$pct_protein[i] <- macros2$avg_protein[i]/rowSums(macros2[,c(2:4)])[i]
#   macros2$pct_fat[i] <- macros2$avg_fat[i]/rowSums(macros2[,c(2:4)])[i]
#   macros2$pct_carb[i] <- macros2$avg_carb[i]/rowSums(macros2[,c(2:4)])[i]
# }
# 
# 


macros2 = na.omit(macros2)

```

# Produce complete dataset

```{r}
dat1 = dat %>% select(c(1:53))
final <- merge(dat1, macros2, by = "ABSPID")

final_with_vars <- final %>% select(BMISC,
                                    AGEC,
                                    SEX,
                                    avg_protein,
                                    avg_fat,
                                    avg_carb
                                    )


final_with_vars = final_with_vars %>% na.omit()
save(final_with_vars, file = "final_with_vars.Rdata")
```

# linear model


## assumptions

```{r}
library(GGally)
numerics = final_with_vars %>% select(-SEX)

ggpairs(numerics)
```


all the plots look fairly linear, there is no clear violations. age has a moderate positive correlation with bmi (0.499)

## model form

```{r}
model_full <- lm(BMISC ~ ., data = final_with_vars)

model_full %>% summary()
```

$$
\operatorname{\widehat{BMISC}} = 20.14 + 0.14(\operatorname{AGEC}) - 0.32(\operatorname{SEX}_{\operatorname{2}}) + 0.0063711 (\operatorname{avg\_protein}) + -0.0003906(\operatorname{avg\_fat}) + -0.0017331(\operatorname{avg\_carb})
$$

From this we can see that protein and carb still are significant after adjusting for age and sex.

### diagnostic plots

```{r}
library(ggfortify)
autoplot(model_full)
```

residuals v fitted and scale v location look pretty random so heteroscedasticity is ok.

normal QQ has deviation at tails, but very large sample size so CLT will apply so is ok.

# model selection

```{r}
model_null = lm(BMISC ~ 1, data = final_with_vars)

step(model_null, scope = list(lower = model_null, upper = model_full), direction = "forward", trace = F)
step(model_full, direction = "backward", trace = F)
```

we should drop avg_fat both forward and back selection do that

# 5 fold cv with full model


```{r}
library(caret)
set.seed(10)

cv_full = train(
  BMISC ~ AGEC + SEX + avg_protein + avg_fat + avg_carb,
  data = final_with_vars,
  method = "lm",
  trControl = trainControl(
    method = "cv", number = 5,
    verboseIter = FALSE
  )
)
cv_full

# cv selected model
cv_selection = train(
  BMISC ~ AGEC + SEX + avg_protein + avg_carb,
  data = final_with_vars,
  method = "lm",
  trControl = trainControl(
    method = "cv", number = 5,
    verboseIter = FALSE
  )
)
cv_selection

```
```{r}
library(caret)
set.seed(10)
params = trainControl(method = "cv", number = 10, verboseIter = FALSE)

cv_objects = list(
  M_full = train(BMISC ~ AGEC + SEX + avg_protein + avg_fat + avg_carb, method = "lm", data = final_with_vars, trControl = params),
  M_back = train(BMISC ~ AGEC + SEX + avg_protein + avg_carb, method = "lm", data = final_with_vars, trControl = params)
)


cv_results = resamples(cv_objects, metric = "Rsquared")
ggplot(cv_results) +
  theme_bw() +
  labs(x = "Models", y = "Mean Absolute Error", title = "10-Fold CV Performance")
```

selection model has almost exact same performance, but significnatly less error, so it is the better performing model.



